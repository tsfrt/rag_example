{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa271996-8de0-4467-bdf6-50d531ee41b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": "brjjdkgknvktnrkffrtkieunlenktvricguk"
    }
   },
   "outputs": [],
   "source": [
    "%pip install --quiet databricks-sdk httpx PyMuPDF openai\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa0c7d37-95d2-437b-917f-20c12a2ce072",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import base64\n",
    "import fitz \n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import time\n",
    "import random\n",
    "import threading\n",
    "from collections import deque\n",
    "from datetime import datetime, timedelta\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from pyspark.sql.functions import col, concat, lit, regexp_replace, split\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac2acf1f-729c-4ac4-bd31-faf8e4998454",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.text(\n",
    "    \"volume_path\",\n",
    "    \"/Volumes/tsfrt/gsa/performance\",\n",
    "    label=\"Path to volume containing documents\",\n",
    ")\n",
    "\n",
    "# MLflow experiment name.\n",
    "dbutils.widgets.text(\n",
    "    \"output_schema\",\n",
    "    \"tsfrt.gsa\",\n",
    "    label=\"Catalog and schema name for output table ({catalog}.{schema})\",\n",
    ")\n",
    "\n",
    "dbutils.widgets.text(\n",
    "    \"output_catalog\",\n",
    "    \"tsfrt\",\n",
    "    label=\"table for final output with embeddings\",\n",
    ")\n",
    "\n",
    "dbutils.widgets.text(\n",
    "    \"output_schema\",\n",
    "    \"gsa\",\n",
    "    label=\"table for final output with embeddings\",\n",
    ")\n",
    "\n",
    "dbutils.widgets.text(\n",
    "    \"output_table\",\n",
    "    \"document_base\",\n",
    "    label=\"table for final output with embeddings\",\n",
    ")\n",
    "\n",
    "dbutils.widgets.text(\n",
    "    \"embedding_model\",\n",
    "    \"databricks-gte-large-en\",\n",
    "    label=\"embedding model to use\",\n",
    ")\n",
    "\n",
    "dbutils.widgets.text(\n",
    "    \"foundation_model\",\n",
    "    \"databricks-llama-4-maverick\",\n",
    "    label=\"foundation model used for doc parsing\",\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44acdadf-92c5-4fea-8246-f6085751e817",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# UPDATE THESE PATHS FOR YOUR SETUP\n",
    "OUTPUT_CTLG = dbutils.widgets.get(\"output_catalog\")\n",
    "OUTPUT_SCHEMA = dbutils.widgets.get(\"output_schema\")\n",
    "OUTPUT_TABLE = dbutils.widgets.get(\"output_table\")\n",
    "\n",
    "PDF_DIRECTORY = dbutils.widgets.get(\"volume_path\")\n",
    "OUTPUT_CATALOG = f\"{OUTPUT_CTLG}.{OUTPUT_SCHEMA}\"\n",
    "\n",
    "# You can choose the processing mode:\n",
    "# \"combined\" - All PDFs go into one table with doc_id to distinguish (recommended)\n",
    "# \"separate\" - Each PDF gets its own table\n",
    "PROCESSING_MODE = \"combined\"  # or \"separate\"\n",
    "\n",
    "# Table naming\n",
    "if PROCESSING_MODE == \"combined\":\n",
    "    INTERMEDIATE_TABLE = f\"{OUTPUT_CATALOG}.all_pdfs_parsed_intermediate\"\n",
    "    FINAL_TABLE = f\"{OUTPUT_CATALOG}.all_pdfs_parsed\"\n",
    "else:\n",
    "    # For separate mode, tables will be named dynamically per PDF\n",
    "    pass\n",
    "\n",
    "context = dbutils.notebook.entry_point.getDbutils().notebook().getContext()\n",
    "workspace_url = context.apiUrl().get()\n",
    "DATABRICKS_TOKEN = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().get()\n",
    "DATABRICKS_BASE_URL = f'{workspace_url}/serving-endpoints/'\n",
    "\n",
    "print(f\"📂 PDF Directory: {PDF_DIRECTORY}\")\n",
    "print(f\"📊 Output Catalog: {OUTPUT_CATALOG}\")\n",
    "print(f\"🔧 Processing Mode: {PROCESSING_MODE}\")\n",
    "if PROCESSING_MODE == \"combined\":\n",
    "    print(f\"💾 Final Table: {FINAL_TABLE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9772c304-e711-4de4-b14a-c5ef2c8495a4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "bbrhbkjgfhjrbub"
    }
   },
   "outputs": [],
   "source": [
    "def count_pdf_pages_fitz(directory=\".\", show_details=True):\n",
    "    \"\"\"\n",
    "    Count pages in all PDF files in a directory using PyMuPDF (fitz)\n",
    "    \n",
    "    Args:\n",
    "        directory (str): Directory path to scan for PDFs\n",
    "        show_details (bool): Whether to show individual file counts\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (total_pages, file_count, errors)\n",
    "    \"\"\"\n",
    "    pdf_files = list(Path(directory).glob(\"*.pdf\"))\n",
    "    \n",
    "    if not pdf_files:\n",
    "        print(f\"No PDF files found in '{directory}'\")\n",
    "        return 0, 0, 0\n",
    "    \n",
    "    total_pages = 0\n",
    "    file_count = 0\n",
    "    errors = 0\n",
    "    \n",
    "    print(f\"Scanning {len(pdf_files)} PDF files in '{directory}'...\\n\")\n",
    "    \n",
    "    for pdf_file in pdf_files:\n",
    "        try:\n",
    "            # Open PDF with fitz\n",
    "            doc = fitz.open(pdf_file)\n",
    "            pages = doc.page_count\n",
    "            doc.close()\n",
    "            \n",
    "            if show_details:\n",
    "                print(f\"{pdf_file.name:<50} {pages:>6} pages\")\n",
    "            \n",
    "            total_pages += pages\n",
    "            file_count += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"ERROR - {pdf_file.name}: {e}\")\n",
    "            errors += 1\n",
    "    print(\"\\n\")\n",
    "    return total_pages, file_count, errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ead9553e-6c1c-49ee-974b-54e1c30f9a09",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "total_pages, num_documents, errors = (count_pdf_pages_fitz(PDF_DIRECTORY, show_details=True))\n",
    "\n",
    "print(f\"Total {num_documents} PDFs found with a total of {total_pages} pages. {errors} errors.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d193d82-eb4d-4584-b24c-af20e1ce6fa2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_pdf_files(directory_path):\n",
    "    \"\"\"\n",
    "    Get all PDF files from a Unity Catalog volume directory.\n",
    "    \n",
    "    Args:\n",
    "        directory_path: Path to directory containing PDFs\n",
    "        \n",
    "    Returns:\n",
    "        List of PDF file paths\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # List all files in the directory\n",
    "        files = dbutils.fs.ls(directory_path)\n",
    "        \n",
    "        # Filter for PDF files and clean the paths\n",
    "        pdf_files = []\n",
    "        for file in files:\n",
    "            if file.path.lower().endswith('.pdf'):\n",
    "                # Remove 'dbfs:' prefix if present to work with PyMuPDF\n",
    "                clean_path = file.path.replace('dbfs:', '') if file.path.startswith('dbfs:') else file.path\n",
    "                pdf_files.append(clean_path)\n",
    "        \n",
    "        print(f\"Found {len(pdf_files)} PDF files in {directory_path}\")\n",
    "        for pdf in pdf_files:\n",
    "            file_name = os.path.basename(pdf)\n",
    "            print(f\"  - {file_name}\")\n",
    "            print(f\"    Path: {pdf}\")\n",
    "            \n",
    "        return pdf_files\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error accessing directory {directory_path}: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "def get_clean_doc_name(pdf_path):\n",
    "    \"\"\"Extract a clean document name from the PDF path for table naming.\"\"\"\n",
    "    file_name = os.path.basename(pdf_path)\n",
    "    # Remove .pdf extension and clean up for table naming\n",
    "    clean_name = file_name.replace('.pdf', '').replace('.PDF', '')\n",
    "    # Replace special characters with underscores\n",
    "    clean_name = ''.join(c if c.isalnum() else '_' for c in clean_name)\n",
    "    # Remove consecutive underscores and strip\n",
    "    clean_name = '_'.join(filter(None, clean_name.split('_')))\n",
    "    return clean_name.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7ed2ecc-bd06-4f09-8666-2ae6a9a8d18f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def convert_pdf_to_base64(pdf_path, dpi=300):\n",
    "    \"\"\"\n",
    "    PDF conversion with better metadata and error handling.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path: Path to PDF file\n",
    "        dpi: Resolution\n",
    "    \n",
    "    Returns:\n",
    "        pandas DataFrame with metadata, success boolean, error message\n",
    "    \"\"\"\n",
    "    \n",
    "    zoom = dpi / 72\n",
    "    zoom_matrix = fitz.Matrix(zoom, zoom)\n",
    "    \n",
    "    try:\n",
    "        doc = fitz.open(pdf_path)\n",
    "        num_pages = len(doc)\n",
    "        \n",
    "        # Extract document metadata\n",
    "        metadata = doc.metadata\n",
    "        file_name = os.path.basename(pdf_path)\n",
    "        clean_doc_name = get_clean_doc_name(pdf_path)\n",
    "        \n",
    "        print(f\"Converting {file_name} to base64: {num_pages} pages at {dpi} DPI...\")\n",
    "        \n",
    "        df_data = []\n",
    "        start_time = time.time()\n",
    "        \n",
    "        for page_num in range(num_pages):\n",
    "            if page_num % 25 == 0:  # Progress update every 25 pages\n",
    "                print(f\"  Converting page {page_num + 1}/{num_pages} to base64\")\n",
    "            \n",
    "            page = doc.load_page(page_num)\n",
    "            \n",
    "            # Get page dimensions and text for metadata\n",
    "            page_rect = page.rect\n",
    "            page_text_length = len(page.get_text())\n",
    "            \n",
    "            pix = page.get_pixmap(matrix=zoom_matrix, alpha=False)\n",
    "            img_bytes = pix.tobytes(\"png\")  \n",
    "            img_base64 = base64.b64encode(img_bytes).decode('utf-8')\n",
    "            \n",
    "            df_data.append({\n",
    "                'doc_id': pdf_path,\n",
    "                'doc_name': clean_doc_name,\n",
    "                'file_name': file_name,\n",
    "                'page_num': page_num + 1,\n",
    "                'total_pages': num_pages,\n",
    "                'page_width': page_rect.width,\n",
    "                'page_height': page_rect.height,\n",
    "                'page_text_length': page_text_length,\n",
    "                'base64_img': img_base64,\n",
    "                'processed_timestamp': datetime.now(),\n",
    "                'dpi': dpi,\n",
    "                'doc_title': metadata.get('title', ''),\n",
    "                'doc_author': metadata.get('author', ''),\n",
    "                'doc_subject': metadata.get('subject', ''),\n",
    "                'doc_creator': metadata.get('creator', '')\n",
    "            })\n",
    "        \n",
    "        doc.close()\n",
    "        processing_time = time.time() - start_time\n",
    "        \n",
    "        print(f\"  Conversion complete: {len(df_data)} pages in {processing_time:.1f}s\")\n",
    "        \n",
    "        return pd.DataFrame(df_data), True, None\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = f\"Error processing {pdf_path}: {str(e)}\"\n",
    "        print(f\"❌ {error_msg}\")\n",
    "        return None, False, error_msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1946b9fd-33f4-4d27-b1cf-58bb5a76855d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def save_to_unity_catalog(df, table_path, mode=\"append\"):\n",
    "    \"\"\"\n",
    "    Save function with better error handling and options.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        spark_df = spark.createDataFrame(df)\n",
    "        \n",
    "        if mode == \"overwrite\":\n",
    "            spark_df.write \\\n",
    "                .format(\"delta\") \\\n",
    "                .mode(\"overwrite\") \\\n",
    "                .option(\"overwriteSchema\", \"true\") \\\n",
    "                .saveAsTable(table_path)\n",
    "        else:\n",
    "            spark_df.write \\\n",
    "                .format(\"delta\") \\\n",
    "                .mode(\"append\") \\\n",
    "                .saveAsTable(table_path)\n",
    "        \n",
    "        print(f\"✅ Saved {len(df)} records to: {table_path}\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error saving to {table_path}: {str(e)}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "213d83a8-cb93-41be-8dbb-78f02772bad5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "RETRYABLE_ERROR_SUBSTRINGS = [\"retry\", \"got empty embedding result\", \"request_limit_exceeded\", \"rate limit\", \"insufficient_quota\", \"expecting value\", \"rate\", \"overloaded\", \"429\", \"bad gateway\", \"502\"]\n",
    "\n",
    "class RateLimitTracker:\n",
    "    \"\"\"Track API rate limits and adjust concurrency dynamically.\"\"\"\n",
    "    \n",
    "    def __init__(self, initial_workers=5, min_workers=1, max_workers=10):\n",
    "        self.current_workers = initial_workers\n",
    "        self.min_workers = min_workers\n",
    "        self.max_workers = max_workers\n",
    "        self.rate_limit_events = deque(maxlen=20)  # Track recent rate limits\n",
    "        self.success_count = 0\n",
    "        self.lock = threading.Lock()\n",
    "        \n",
    "    def record_rate_limit(self):\n",
    "        \"\"\"Record a rate limit event and potentially reduce workers.\"\"\"\n",
    "        with self.lock:\n",
    "            self.rate_limit_events.append(datetime.now())\n",
    "            \n",
    "            # If we've had multiple rate limits recently, reduce workers\n",
    "            recent_limits = sum(1 for event in self.rate_limit_events \n",
    "                              if datetime.now() - event < timedelta(minutes=2))\n",
    "            \n",
    "            if recent_limits >= 3 and self.current_workers > self.min_workers:\n",
    "                old_workers = self.current_workers\n",
    "                self.current_workers = max(self.min_workers, self.current_workers - 1)\n",
    "                print(f\"🔽 Rate limits detected! Reducing workers: {old_workers} → {self.current_workers}\")\n",
    "                \n",
    "    def record_success(self):\n",
    "        \"\"\"Record successful processing and potentially increase workers.\"\"\"\n",
    "        with self.lock:\n",
    "            self.success_count += 1\n",
    "            \n",
    "            # If no recent rate limits and we've had some successes, gradually increase workers\n",
    "            recent_limits = sum(1 for event in self.rate_limit_events \n",
    "                              if datetime.now() - event < timedelta(minutes=5))\n",
    "            \n",
    "            # Increase workers every 20 successes if no recent rate limits\n",
    "            if (recent_limits == 0 and \n",
    "                self.current_workers < self.max_workers and \n",
    "                self.success_count % 20 == 0):\n",
    "                old_workers = self.current_workers\n",
    "                self.current_workers = min(self.max_workers, self.current_workers + 1)\n",
    "                print(f\"🔼 Performance good! Increasing workers: {old_workers} → {self.current_workers}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1df8c680-f2af-473b-be01-73817201c1ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def process_single_image(prompt, image_data, image_index, databricks_token, databricks_url, model, rate_tracker):\n",
    "    \"\"\"Process a single image with adaptive rate limiting.\"\"\"\n",
    "    \n",
    "    client = OpenAI(api_key=databricks_token, base_url=databricks_url)\n",
    "    \n",
    "    # Skip empty images\n",
    "    if pd.isna(image_data) or image_data == \"\":\n",
    "        return (image_index, \"ERROR: Empty image\")\n",
    "    \n",
    "    \n",
    "    # Retry logic with exponential backoff\n",
    "    for attempt in range(3):\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=[{\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\"type\": \"text\", \"text\": prompt},\n",
    "                        {\n",
    "                            \"type\": \"image_url\",\n",
    "                            \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image_data}\"}\n",
    "                        }\n",
    "                    ]\n",
    "                }]\n",
    "            )\n",
    "            \n",
    "            result = response.choices[0].message.content.strip()\n",
    "            rate_tracker.record_success()\n",
    "            \n",
    "            # Print success message if this was a retry attempt\n",
    "            if attempt > 0:\n",
    "                print(f\"✅ SUCCESS: Image {image_index} processed successfully after {attempt + 1} attempts\")\n",
    "            \n",
    "            return (image_index, result)\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_str = str(e).lower()\n",
    "            is_retryable = any(substring in error_str for substring in RETRYABLE_ERROR_SUBSTRINGS)\n",
    "            \n",
    "            if is_retryable:\n",
    "                rate_tracker.record_rate_limit()\n",
    "                \n",
    "                if attempt < 2:  # Only retry if we have attempts left\n",
    "                    # Exponential backoff with jitter\n",
    "                    wait_time = (2 ** attempt) + random.uniform(1, 3)\n",
    "                    print(f\"⚠️  RATE LIMIT: Image {image_index}, attempt {attempt + 1}/3. Retrying in {wait_time:.1f}s...\")\n",
    "                    time.sleep(wait_time)\n",
    "                    continue\n",
    "                else:\n",
    "                    print(f\"❌ FAILED: Image {image_index} failed after 3 attempts due to rate limiting\")\n",
    "                    return (image_index, f\"ERROR: Rate limited after 3 attempts - {str(e)}\")\n",
    "            else:\n",
    "                print(f\"❌ ERROR: Image {image_index} failed with non-retryable error: {str(e)}\")\n",
    "                return (image_index, f\"ERROR: {str(e)}\")\n",
    "    \n",
    "    return (image_index, \"ERROR: Max retries exceeded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7aec551-7d74-41c6-8fb5-e28bfb9093e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def process_images_adaptive(prompt, images, databricks_token, databricks_url, \n",
    "                           model=\"databricks-llama-4-maverick\", \n",
    "                           initial_workers=5, min_workers=1, max_workers=10):\n",
    "    \"\"\"\n",
    "    Adaptive processing that adjusts concurrency based on rate limits.\n",
    "    \n",
    "    Args:\n",
    "        images: pandas Series of base64 encoded image strings\n",
    "        databricks_token: Token for Databricks API  \n",
    "        databricks_url: Base URL for Databricks API\n",
    "        model: Model name to use\n",
    "        initial_workers: Starting number of concurrent workers\n",
    "        min_workers: Minimum workers (fallback during heavy rate limiting)\n",
    "        max_workers: Maximum workers (cap for scaling up)\n",
    "        \n",
    "    Returns:\n",
    "        pandas Series: Results with same index as input\n",
    "    \"\"\"\n",
    "    \n",
    "    # Convert to pandas Series if needed\n",
    "    if not isinstance(images, pd.Series):\n",
    "        images = pd.Series(images)\n",
    "    \n",
    "    results = pd.Series(index=images.index, dtype='object')\n",
    "    rate_tracker = RateLimitTracker(\n",
    "        initial_workers=initial_workers, \n",
    "        min_workers=min_workers, \n",
    "        max_workers=max_workers\n",
    "    )\n",
    "    \n",
    "    print(f\"🚀 Starting transcription of {len(images)} images...\")\n",
    "    print(f\"📊 Model: {model}\")\n",
    "    print(f\"⚙️  Workers: {initial_workers} (range: {min_workers}-{max_workers})\")\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        with tqdm(total=len(images), desc=\"Processing images\", unit=\"img\") as pbar:\n",
    "            \n",
    "            remaining_items = list(images.items())\n",
    "            \n",
    "            while remaining_items:\n",
    "                # Submit batch based on current worker count\n",
    "                batch_size = min(rate_tracker.current_workers, len(remaining_items))\n",
    "                current_batch = remaining_items[:batch_size]\n",
    "                remaining_items = remaining_items[batch_size:]\n",
    "                \n",
    "                # Submit current batch\n",
    "                futures = {\n",
    "                    executor.submit(process_single_image, prompt, img_data, idx, \n",
    "                                  databricks_token, databricks_url, model, rate_tracker): idx\n",
    "                    for idx, img_data in current_batch\n",
    "                }\n",
    "                \n",
    "                # Process batch results\n",
    "                for future in as_completed(futures):\n",
    "                    try:\n",
    "                        image_index, result = future.result()\n",
    "                        results[image_index] = result\n",
    "                        \n",
    "                        # Update progress bar with status and current worker count\n",
    "                        if result.startswith(\"ERROR:\"):\n",
    "                            pbar.set_postfix({\n",
    "                                \"Last\": f\"❌ {image_index}\", \n",
    "                                \"Workers\": rate_tracker.current_workers,\n",
    "                                \"Rate Limits\": len(rate_tracker.rate_limit_events)\n",
    "                            })\n",
    "                        else:\n",
    "                            pbar.set_postfix({\n",
    "                                \"Last\": f\"✅ {image_index}\", \n",
    "                                \"Workers\": rate_tracker.current_workers,\n",
    "                                \"Rate Limits\": len(rate_tracker.rate_limit_events)\n",
    "                            })\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        idx = futures[future]\n",
    "                        results[idx] = f\"ERROR: Exception - {str(e)}\"\n",
    "                        pbar.set_postfix({\n",
    "                            \"Last\": f\"❌ {idx} (Exception)\", \n",
    "                            \"Workers\": rate_tracker.current_workers\n",
    "                        })\n",
    "                        print(f\"❌ EXCEPTION: Image {idx} failed with exception: {str(e)}\")\n",
    "                    \n",
    "                    pbar.update(1)\n",
    "                \n",
    "                # Small delay between batches if we have more to process\n",
    "                if remaining_items:\n",
    "                    time.sleep(0.2)  # Small delay to prevent overwhelming\n",
    "    \n",
    "    # Final summary statistics\n",
    "    error_count = sum(1 for result in results if str(result).startswith(\"ERROR:\"))\n",
    "    success_count = len(results) - error_count\n",
    "    \n",
    "    print(f\"\\n📈 Llama 4 Transcription Summary:\")\n",
    "    print(f\"   ✅ Successful: {success_count}/{len(results)}\")\n",
    "    print(f\"   ❌ Failed: {error_count}/{len(results)}\")\n",
    "    print(f\"   📊 Success rate: {(success_count/len(results)*100):.1f}%\")\n",
    "    print(f\"   🔧 Final worker count: {rate_tracker.current_workers}\")\n",
    "    print(f\"   ⚠️  Total rate limit events: {len(rate_tracker.rate_limit_events)}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ffb93f86-4251-4a13-91c7-08db80884d99",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Tweak your prompt based on document content as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a1a508f-2876-40fa-94a9-0d6d3509148f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "    # Define the prompt\n",
    "PROMPT = \"\"\"\n",
    "Instructions: Transcribe only the visible text from this PDF page. \n",
    "Rules:\n",
    "- Use markdown formatting only for text that appears formatted in the original\n",
    "- Do not add document titles, page headers, or section headings unless explicitly visible\n",
    "- Do not add introductory text like 'This page contains...' or 'The document shows...' or '# Transcription of PDF Page'\n",
    "- Preserve exact wording and technical terminology\n",
    "- For images/diagrams: describe content within <figure></figure> tags\n",
    "- For tables: use markdown table format if present\n",
    "- Start transcription immediately without preamble\n",
    "For visual elements, follow these rules:\n",
    "**TABLES**: If the content is clearly a structured table, provide BOTH:\n",
    "1. A detailed caption in <figure></figure> tags describing the table structure and content\n",
    "2. The actual table recreated in markdown format with proper alignment\n",
    "**FLOWCHARTS/DECISION TREES**: Provide detailed caption in <figure></figure> tags including:\n",
    "- Starting point and decision criteria\n",
    "- All pathways and decision branches\n",
    "- Specific thresholds, values, and conditions\n",
    "- Final outcomes and recommendations\n",
    "- Flow direction and logical connections\n",
    "**CHARTS/DIAGRAMS**: Provide detailed caption in <figure></figure> tags including:\n",
    "- Chart type and title\n",
    "- All categories, sections, and color coding\n",
    "- Specific values, ranges, and criteria\n",
    "- Evidence levels and recommendations\n",
    "- Visual organization and groupings\n",
    "**FORMS/CHECKLISTS**: Transcribe structure using markdown formatting, preserving:\n",
    "- Section headers and numbering\n",
    "- Checkbox options and rating scales\n",
    "- Please bold the Key in Key-Value Pairs in the form, e.g. **Name **: John Doe.\n",
    "Preserve exact technical terminology, drug names, dosages, and clinical criteria for diagnostic accuracy.\n",
    "This transcription will be used for technical diagnosis, so accuracy is critical.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4f1b6ae-b3f6-4ae0-af34-5559895c21eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def process_multiple_pdfs(pdf_directory, output_catalog, prompt=PROMPT, processing_mode=\"combined\", \n",
    "                         dpi=300, model=\"databricks-llama-4-maverick\", initial_workers=5, \n",
    "                         min_workers=1, max_workers=10):\n",
    "    \"\"\"\n",
    "    Process all PDFs in a directory.\n",
    "    \n",
    "    Args:\n",
    "        pdf_directory: Directory containing PDF files\n",
    "        output_catalog: Catalog.schema for output tables\n",
    "        processing_mode: \"combined\" or \"separate\"\n",
    "        dpi: Image resolution\n",
    "        model: LLM model to use\n",
    "    \"\"\"\n",
    "    \n",
    "    # Discover PDF files\n",
    "    pdf_files = get_pdf_files(pdf_directory)\n",
    "    \n",
    "    if not pdf_files:\n",
    "        print(\"No PDF files found. Exiting.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\n🚀 Starting batch processing of {len(pdf_files)} PDFs\")\n",
    "    print(f\"📊 Processing mode: {processing_mode}\")\n",
    "    print(f\"🎯 Output catalog: {output_catalog}\")\n",
    "    \n",
    "    # Initialize tracking variables\n",
    "    total_files = len(pdf_files)\n",
    "    successful_files = 0\n",
    "    failed_files = 0\n",
    "    total_pages_processed = 0\n",
    "    all_results = []\n",
    "    processing_log = []\n",
    "    \n",
    "    # Process each PDF\n",
    "    for file_idx, pdf_path in enumerate(pdf_files, 1):\n",
    "        file_name = os.path.basename(pdf_path)\n",
    "        clean_doc_name = get_clean_doc_name(pdf_path)\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"📄 Processing file {file_idx}/{total_files}: {file_name}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        file_start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            # Convert PDF to base64 images\n",
    "            df, success, error = convert_pdf_to_base64(pdf_path, dpi=dpi)\n",
    "            \n",
    "            if not success:\n",
    "                failed_files += 1\n",
    "                processing_log.append({\n",
    "                    'file_name': file_name,\n",
    "                    'status': 'FAILED_CONVERSION',\n",
    "                    'error': error,\n",
    "                    'pages_processed': 0,\n",
    "                    'processing_time': time.time() - file_start_time\n",
    "                })\n",
    "                continue\n",
    "            \n",
    "            # Save intermediate results\n",
    "            if processing_mode == \"combined\":\n",
    "                intermediate_table = f\"{output_catalog}.all_pdfs_parsed_intermediate\"\n",
    "                save_mode = \"append\" if file_idx > 1 else \"overwrite\"\n",
    "            else:\n",
    "                intermediate_table = f\"{output_catalog}.{clean_doc_name}_parsed_intermediate\"\n",
    "                save_mode = \"overwrite\"\n",
    "                \n",
    "            save_to_unity_catalog(df, intermediate_table, mode=save_mode)\n",
    "            \n",
    "            # Process images with LLM\n",
    "            print(f\"🤖 Starting LLM processing for {len(df)} pages...\")\n",
    "            \n",
    "            # Process with adaptive rate limiting\n",
    "            results_series = process_images_adaptive(\n",
    "                prompt=prompt,\n",
    "                images=df['base64_img'],\n",
    "                databricks_token=DATABRICKS_TOKEN,\n",
    "                databricks_url=DATABRICKS_BASE_URL,\n",
    "                model=model,\n",
    "                initial_workers=initial_workers,\n",
    "                min_workers=min_workers,\n",
    "                max_workers=max_workers\n",
    "            )\n",
    "            \n",
    "            # Add transcription results to dataframe\n",
    "            df['transcription'] = results_series\n",
    "            \n",
    "            # Count successful transcriptions\n",
    "            error_count = sum(1 for result in results_series if str(result).startswith(\"ERROR:\"))\n",
    "            success_count = len(results_series) - error_count\n",
    "            \n",
    "            # Save final results\n",
    "            if processing_mode == \"combined\":\n",
    "                final_table = f\"{output_catalog}.all_pdfs_parsed\"\n",
    "                save_mode = \"append\" if file_idx > 1 else \"overwrite\"\n",
    "            else:\n",
    "                final_table = f\"{output_catalog}.{clean_doc_name}_parsed\"\n",
    "                save_mode = \"overwrite\"\n",
    "                \n",
    "            save_success = save_to_unity_catalog(df, final_table, mode=save_mode)\n",
    "            \n",
    "            if save_success:\n",
    "                successful_files += 1\n",
    "                total_pages_processed += len(df)\n",
    "                all_results.append(df)\n",
    "                \n",
    "                file_processing_time = time.time() - file_start_time\n",
    "                \n",
    "                processing_log.append({\n",
    "                    'file_name': file_name,\n",
    "                    'status': 'SUCCESS',\n",
    "                    'pages_processed': len(df),\n",
    "                    'successful_transcriptions': success_count,\n",
    "                    'failed_transcriptions': error_count,\n",
    "                    'processing_time': file_processing_time,\n",
    "                    'final_table': final_table\n",
    "                })\n",
    "                \n",
    "                print(f\"✅ File completed successfully:\")\n",
    "                print(f\"   📊 Pages: {len(df)}\")\n",
    "                print(f\"   ✅ Successful transcriptions: {success_count}\")\n",
    "                print(f\"   ❌ Failed transcriptions: {error_count}\")\n",
    "                print(f\"   ⏱️  Processing time: {file_processing_time:.1f}s\")\n",
    "                print(f\"   💾 Saved to: {final_table}\")\n",
    "            else:\n",
    "                failed_files += 1\n",
    "                processing_log.append({\n",
    "                    'file_name': file_name,\n",
    "                    'status': 'FAILED_SAVE',\n",
    "                    'pages_processed': len(df),\n",
    "                    'processing_time': time.time() - file_start_time\n",
    "                })\n",
    "                \n",
    "        except Exception as e:\n",
    "            failed_files += 1\n",
    "            file_processing_time = time.time() - file_start_time\n",
    "            error_msg = str(e)\n",
    "            \n",
    "            processing_log.append({\n",
    "                'file_name': file_name,\n",
    "                'status': 'FAILED_EXCEPTION',\n",
    "                'error': error_msg,\n",
    "                'pages_processed': 0,\n",
    "                'processing_time': file_processing_time\n",
    "            })\n",
    "            \n",
    "            print(f\"❌ Failed to process {file_name}: {error_msg}\")\n",
    "    \n",
    "    # Final summary\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"🎊 BATCH PROCESSING COMPLETE\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"📊 Files processed: {successful_files}/{total_files}\")\n",
    "    print(f\"📄 Total pages processed: {total_pages_processed}\")\n",
    "    print(f\"✅ Successful files: {successful_files}\")\n",
    "    print(f\"❌ Failed files: {failed_files}\")\n",
    "    \n",
    "    if processing_mode == \"combined\" and successful_files > 0:\n",
    "        print(f\"💾 All results combined in: {output_catalog}.all_pdfs_parsed\")\n",
    "    \n",
    "    # Show processing log\n",
    "    print(f\"\\n📋 PROCESSING LOG:\")\n",
    "    for log_entry in processing_log:\n",
    "        status_emoji = \"✅\" if log_entry['status'] == 'SUCCESS' else \"❌\"\n",
    "        print(f\"   {status_emoji} {log_entry['file_name']}: {log_entry['status']} \"\n",
    "              f\"({log_entry['pages_processed']} pages, {log_entry['processing_time']:.1f}s)\")\n",
    "        \n",
    "        if 'error' in log_entry:\n",
    "            print(f\"      Error: {log_entry['error']}\")\n",
    "    \n",
    "    return processing_log, all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ad3564f-3046-4630-9582-9f42df423727",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Run the batch processing\n",
    "processing_log, all_results = process_multiple_pdfs(\n",
    "    pdf_directory=PDF_DIRECTORY,\n",
    "    output_catalog=OUTPUT_CATALOG,\n",
    "    prompt = PROMPT,\n",
    "    processing_mode=PROCESSING_MODE,\n",
    "    dpi=150,\n",
    "    model=\"databricks-llama-4-maverick\", #default databricks-llama-4-maverick, change to your own provisioned throughput endpoint for more speed\n",
    "    initial_workers=3, #update if you have a provisioned throughput endpoint\n",
    "    min_workers=1, #default 1\n",
    "    max_workers=3 #update if you have a provisioned throughput endpoint\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a449586b-cd8c-4d83-a844-21400da249b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# If using combined mode, show summary statistics\n",
    "if PROCESSING_MODE == \"combined\":\n",
    "    summary_df = spark.sql(f\"\"\"\n",
    "        SELECT \n",
    "            file_name,\n",
    "            doc_name,\n",
    "            COUNT(*) as total_pages,\n",
    "            SUM(CASE WHEN transcription NOT LIKE 'ERROR:%' THEN 1 ELSE 0 END) as successful_pages,\n",
    "            SUM(CASE WHEN transcription LIKE 'ERROR:%' THEN 1 ELSE 0 END) as failed_pages,\n",
    "            AVG(page_text_length) as avg_page_text_length,\n",
    "            MIN(processed_timestamp) as first_processed,\n",
    "            MAX(processed_timestamp) as last_processed\n",
    "        FROM {FINAL_TABLE}\n",
    "        GROUP BY file_name, doc_name\n",
    "        ORDER BY file_name\n",
    "    \"\"\")\n",
    "    \n",
    "    print(\"📊 PROCESSING SUMMARY BY FILE:\")\n",
    "display(summary_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1f72bb9-fe0c-4e1b-8416-523e04d6f6a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"DROP TABLE IF EXISTS {OUTPUT_CTLG}.{OUTPUT_SCHEMA}.{OUTPUT_TABLE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8adc9a6c-40fb-4d2c-bf3a-d8061e075924",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "embedding_model = dbutils.widgets.get(\"embedding_model\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE {OUTPUT_CTLG}.{OUTPUT_SCHEMA}.{OUTPUT_TABLE} as SELECT\n",
    "  ROW_NUMBER() OVER (ORDER BY transcription) as id,\n",
    "  doc_id,\n",
    "  transcription,\n",
    "  ai_query(\"{embedding_model}\", subquery.transcription) as embedding\n",
    "FROM\n",
    "  (\n",
    "    SELECT\n",
    "      doc_id,\n",
    "      transcription\n",
    "    FROM\n",
    "      {OUTPUT_CTLG}.{OUTPUT_SCHEMA}.all_pdfs_parsed\n",
    "  ) AS subquery\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "dbe_41a50460-c90b-4840-9288-afcb847395d5",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "01_document_parse",
   "widgets": {
    "embedding_model": {
     "currentValue": "databricks-gte-large-en",
     "nuid": "e7a18c5d-69a1-4aa7-bb32-7ebcd184bbce",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "databricks-gte-large-en",
      "label": "embedding model to use",
      "name": "embedding_model",
      "options": {
       "validationRegex": null,
       "widgetDisplayType": "Text"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "databricks-gte-large-en",
      "label": "embedding model to use",
      "name": "embedding_model",
      "options": {
       "autoCreated": null,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "foundation_model": {
     "currentValue": "databricks-llama-4-maverick",
     "nuid": "8c25ee5d-0013-4afa-b6e9-ad9af065289d",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "databricks-llama-4-maverick",
      "label": "foundation model used for doc parsing",
      "name": "foundation_model",
      "options": {
       "validationRegex": null,
       "widgetDisplayType": "Text"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "databricks-llama-4-maverick",
      "label": "foundation model used for doc parsing",
      "name": "foundation_model",
      "options": {
       "autoCreated": null,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "output_schema": {
     "currentValue": "tsfrt.gsa",
     "nuid": "5572b69d-c9e7-4d22-a05d-5a65b1dff641",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "tsfrt.gsa",
      "label": "Catalog and schema name for output table ({catalog}.{schema})",
      "name": "output_schema",
      "options": {
       "validationRegex": null,
       "widgetDisplayType": "Text"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "tsfrt.gsa",
      "label": "Catalog and schema name for output table ({catalog}.{schema})",
      "name": "output_schema",
      "options": {
       "autoCreated": null,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "output_table": {
     "currentValue": "tsfrt.gsa.document_base",
     "nuid": "fcb2f541-97fc-4092-a50e-604bed54b3d9",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "tsfrt.gsa.document_base2",
      "label": "table for final output with embeddings",
      "name": "output_table",
      "options": {
       "validationRegex": null,
       "widgetDisplayType": "Text"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "tsfrt.gsa.document_base2",
      "label": "table for final output with embeddings",
      "name": "output_table",
      "options": {
       "autoCreated": null,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "volume_path": {
     "currentValue": "/Volumes/tsfrt/gsa/performance",
     "nuid": "bf3e1256-9f0c-4023-a278-c83732cab929",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "/Volumes/tsfrt/gsa/performance",
      "label": "Path to volume containing documents",
      "name": "volume_path",
      "options": {
       "validationRegex": null,
       "widgetDisplayType": "Text"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "/Volumes/tsfrt/gsa/performance",
      "label": "Path to volume containing documents",
      "name": "volume_path",
      "options": {
       "autoCreated": null,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
